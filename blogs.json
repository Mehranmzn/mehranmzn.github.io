{
  "status": "ok",
  "feed": {
    "url": "https://medium.com/feed/@mehran1414",
    "title": "Stories by Mehran Moazeni on Medium",
    "link": "https://medium.com/@mehran1414?source=rss-3855f4d29cc4------2",
    "author": "",
    "description": "Stories by Mehran Moazeni on Medium",
    "image": "https://cdn-images-1.medium.com/fit/c/150/150/0*0QWe7gC7KlO_s9q1"
  },
  "items": [
    {
      "title": "Churn Prediction in Sparkify dataset",
      "pubDate": "2023-03-24 11:23:33",
      "link": "https://medium.com/@mehran1414/churn-prediction-in-sparkify-dataset-5e61230c338d?source=rss-3855f4d29cc4------2",
      "guid": "https://medium.com/p/5e61230c338d",
      "author": "Mehran Moazeni",
      "thumbnail": "",
      "description": "\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/841/1*jfqNg7QD_g7GVAnL-bv_LQ.jpeg\"></figure><h3>Project motivation</h3>\n<p>One issue that frequently arises in service-based businesses, such as Spotify, is the need to anticipate churn. Churn refers to the departure of customers, and it\u2019s a crucial metric to keep track of. By anticipating churn, these companies can take preventative measures to avoid customer loss, which can ultimately lead to an increase in customer base and\u00a0revenue.</p>\n<p>Udacity has created Sparkify, a simulated music streaming platform that generates data similar to real-world companies like Spotify and Pandora. Each day, millions of users listen to their favorite songs via music streaming services, either on a free tier plan with ads or on a premium subscription model without ads and with additional features. Users can modify their subscription plan at any time or cancel it completely, making it vital to ensure customer satisfaction. Whenever a user interacts with a music streaming app, data is generated, including playing songs, adding them to playlists, rating them, adding friends, logging in/out, and adjusting settings. The user activity logs provide valuable insights into whether users are happy with the\u00a0service.</p>\n<h3>Introduction to\u00a0data</h3>\n<p>As a part of this task, we will be utilizing event data that has been provided to us. There are two datasets at our disposal for analysis, one of considerable size and another that is comparatively smaller. The full dataset contains approximately 12GB of data and could be used by those who wish to deploy their model on AWS or any other cloud service. For the purposes of this task, I will be working with the moderately-sized dataset, which contains 123MB of data. For a more detailed analysis and information on the prerequisites, please refer to <a href=\"https://github.com/Mehranmzn/DataScience-Nanodegree/tree/master/notebooks/Project%204\">my Github</a>\u00a0repo.</p>\n<h3>Exploratory Data\u00a0Analysis</h3>\n<p>To start with, our initial task was to come up with a clear definition of what \u201cchurn\u201d meant for our analysis. This involved creating a new column, called Churn, which served as a label for our model. We determined whether a customer had churned or not based on whether they had accessed the Cancellation Confirmation page, which was included as a feature in our dataset. Specifically, we labeled customers as \u201c1\u201d if they had churned and \u201c0\u201d if they had remained with Sparkify. With this label in place, we could then dive deeper into our data and identify key features that would be most valuable in predicting churn.</p>\n<h4>Frequency of\u00a0Churn</h4>\n<p>The table presented below illustrates that there were a total of 173 Sparkify customers who chose to stay, while 52 customers decided to churn. Therefore, we can deduce that 23% of the subset of Sparkify\u2019s customers had churned. It is worth mentioning that this dataset is imbalanced, which is an important factor to consider as we continue with our analysis.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/166/1*-0nZD-J_lm9t91SGNfNDEw.png\"></figure><h3>Load data and\u00a0Cleaning</h3>\n<p>The dataset consists of 286500 rows and 18 columns, with each user having multiple rows that serve as log entries for each session. While most of the columns are self-explanatory, it\u2019s worth highlighting some crucial features that will be used for our analysis.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/846/1*TnadJ1PN_pOTS3TQMpxZ0w.png\"><figcaption>Columns in the\u00a0data</figcaption></figure><p>Where:</p>\n<p>Artist: The name of the artist whose song was played during the event. This field is null if the event is not related to song listening.</p>\n<p>Authentication (Auth): The status of user authentication, such as login, logout, cancel, or\u00a0guest.</p>\n<p>First Name: The first name of the user. This field is null if the user has not logged\u00a0in.</p>\n<p>Gender: The gender of the user, either male or female. This field is null if the user has not logged\u00a0in.</p>\n<p>Item in Session: The count of events that occurred during a particular session. The count starts from\u00a00.</p>\n<p>Last Name: The last name of the user. This field is null if the user has not logged\u00a0in.</p>\n<p>Length: The duration in seconds of the song being played. This field is null if the event is not related to song listening.</p>\n<p>Level: The subscription level of the user, whether paid or\u00a0free.</p>\n<p>Location: The location of the user in the format of city and state. All users are from the\u00a0USA.</p>\n<p>Page: The page where the event occurred, which can be Cancel, Submit Downgrade, Thumbs Down, Home, Downgrade, Roll Advert, Logout, Save Settings, Cancellation Confirmation, About, Submit Registration, Settings, Login, Register, Add to Playlist, Add Friend, NextSong, Thumbs Up, Help, Upgrade, Error and Submit\u00a0Upgrade.</p>\n<p>Registration: The registration number associated with each\u00a0user.</p>\n<p>Session Id: The number of the session initiated.</p>\n<p>Song: The name of the song being\u00a0played.</p>\n<p>Status: The HTTP status code (200, 307,\u00a0404).</p>\n<p>Timestamp (TS): The timestamp in milliseconds.</p>\n<p>User Agent: Information about the device and browser accessing the\u00a0data.</p>\n<p>User Id: The ID associated with each\u00a0user.</p>\n<p>Upon examining the records, it is observed that a user typically spends a considerable amount of time using the system as a paid user before ultimately ending their subscription. In certain cases, they continue to use the service as a free user. Our objective is to forecast when this event of canceling or ending the subscription will\u00a0occur.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/490/1*Xfl5GhFDXU5H6ajbTQeMbw.png\"><figcaption>Number of artists in the\u00a0dataset</figcaption></figure><p>Churn defined when a user cancel the memebeship from paid to either non-paid or leave the platform.</p>\n<p>To gain a deeper comprehension of churn behavior, we will examine additional columns to identify any significant information within or between them. As a result, I have created a distribution plot of song\u00a0lengths.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/681/1*XSsgQYmS09aoUdckQ1hSJw.png\"><figcaption>Distribution of songs\u00a0length</figcaption></figure><p>Churn rate based on gender, level of membership, and page interaction, given in the below\u00a0figure:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*BAXYaFJhVjIwcCmvZgoPIw.png\"><figcaption>Red bars representing the churn users vs the green bars are non-churned users</figcaption></figure><p>Considering these features, there isn\u2019t a great deal of information to report. Therefore, it would be worthwhile to examine the frequency of page visits for both Churn and non-Churn users.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*zWrRykB5tVbsWn0ePrPSDA.png\"></figure><p>Once again, there doesn\u2019t appear to be a distinct differentiation between Churn and non-Churn users. Next, let\u2019s examine the following figure which illustrates the amount of time spent listening to songs by these two\u00a0groups.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*xxJgLXKyrrnwr1UOc7zd4w.png\"></figure><p>In this case, there is a noticeable distinction between Churn and non-Churn users. It is apparent that users who exhibit Churn behavior listen to a lower quantity of songs compared to the other group. Let\u2019s now shift our attention to the web browsers\u00a0used.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*QMuN6ZioFOKrZVdjBS5bUw.png\"></figure><p>There seems to be more drop out in Chrome than other web browers which could be due to the fact that most of the customer used\u00a0Chrome.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*MCwRPWp4IYESa_0oSMkSBQ.png\"></figure><p>It appears that the average number of days since registration is relatively similar for both groups, but the variance among users in the non-Churn group is\u00a0higher.</p>\n<h3>Feature Engineering</h3>\n<p>I have taken several steps to transform the original dataset to a format that is understandable for modeling part. These steps\u00a0are:</p>\n<ol>\n<li>Gender will be transformed from Male/Female to\u00a0binary</li>\n<li>Level of memebership: e.g., paid will be 1 otherwise 0</li>\n<li>Artists columns: Number of artisits listen to for each unique\u00a0user</li>\n<li>Average number of song a user listen\u00a0to</li>\n<li>Number of times that a user click on the rolling\u00a0ad</li>\n<li>Number of Thums up and Thums down that each user\u00a0did</li>\n<li>Number of times a user add a\u00a0friend</li>\n<li>Number of times a user log out from the\u00a0platform</li>\n<li>Churn will be binary when a Cancellation is confirmed in\u00a0page.</li>\n</ol>\n<p>Once these datasets merged, we have the final, ready to train\u00a0dataset:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*FpXOUux0RDzxSc3_SlTfwQ.png\"></figure><h3>Data preprocessing</h3>\n<p>We need all the columns to be float. So First all the columns type changed to be\u00a0float.</p>\n<h4>Vectorizing</h4>\n<p>Afterward, we employed the VectorAssembler module of Spark to condense the feature space into a vector. Ensuring that features with high values do not overpower the results is crucial for our machine learning\u00a0model.</p>\n<pre>assembler = VectorAssembler(inputCols=['gender', 'level', 'number_artist_listen', 'p_list',<br>                                      'thumbsup', 'thumsdown', 'dgrade', 'nsong', 'ad'],<br>                            outputCol='features_vec')</pre>\n<h4>Standard Scaler</h4>\n<p>The values should be standarized to be sure that they have a similar\u00a0scale.</p>\n<pre># Fit StandardScaler model on assembled data<br>scaler = StandardScaler(inputCol='features_vec', outputCol='scaled_features')<br>scaler_model = scaler.fit(df_assembled)<br><br># Apply scaler model on assembled data<br>df_scaled = scaler_model.transform(df_assembled)</pre>\n<h4>Train-test split</h4>\n<p>Following this, we performed a train-test split of 80/20, which resulted in a training set of 173 and a testing set of\u00a052.</p>\n<h3>Modelling</h3>\n<p>After constructing the features dataFrame with exclusively numeric variables, we proceeded to divide the entire dataset into three subsets: train, test, and validation. We subsequently experimented with various machine learning classification algorithms, including:</p>\n<ul>\n<li>LogisticRegression</li>\n<li>RandomForestClassifier</li>\n<li>GBTClassifier</li>\n<li>NaiveBayes</li>\n</ul>\n<p>In all the methods the target variable (Churn) is a binary variabel so the task is bainary classification.</p>\n<h4>Evaluation Metrics</h4>\n<p>We will assess the performance of the models using two evaluation metrics: accuracy and F1score. F1score is being utilized due to the unbalanced nature of the dataset, with only 52 customers having churned. Our first model to be tested was the tried-and-true logistic regression.</p>\n<h4>LogisticRegression</h4>\n<p>The binary classification problem at hand can benefit from the application of logistic regression, a dependable machine learning algorithm that offers a transparent model. Logistic regression is straightforward to put into practice and interpret, and it boasts a high level of efficiency in training. Additionally, it has a lower tendency to overfit, making it a favorable option.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/940/1*kTvHJHE0fovhfD4-ILFvhw.png\"></figure><h4>RandomForestClassifier</h4>\n<p>The ensemble technique known as Random Forest (RF) builds numerous decision trees to generate predictions, and subsequently aggregates the decisions through a majority vote. By doing so, RF can mitigate the risk of overfitting. Furthermore, RF is a robust algorithm that excels in handling imbalanced datasets, which is pertinent to the current scenario. The performance of RF on such datasets is noteworthy.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1002/1*n2H5c3Allg3ZWaPij_oOPw.png\"></figure><h4>GBTClassifier</h4>\n<p>The Gradient Boosting Trees (GBT) approach constructs decision trees sequentially, with each subsequent tree rectifying the errors of its predecessor. In contrast, Random Forest (RF) creates trees independently. While GBT has the potential to overfit, this must be taken into account. On the positive side, GBT exhibits strong performance in dealing with unbalanced data, which is relevant to the present circumstance.We used several models, including\u00a0, and evaluated them based on accuracy and\u00a0F1score.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/830/1*NouvaVj3sPQnC4QNn3jBsw.png\"></figure><h4>NaiveBayes</h4>\n<p>A fast algorithm for our classfying tasks.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1002/1*DayxofGhxUCUJPdCavBgJQ.png\"></figure><p>The result showed that best performing model was GBTClassifier with accuracy of 0.69 and F1 score of\u00a00.73.</p>\n<h3>Conclusion</h3>\n<p>Our project began with a relatively small dataset of only 128 megabytes (The original dataset of this captone is 12G and becuase of limitation of resources, we worked on a small subset of the data) and 225 distinct customers. Upon importing and refining the data, we conducted a thorough analysis of the dataset to identify pertinent predictors of churn, ultimately isolating the most effective features. These selected features were subjected to preprocessing and subsequently employed as input variables in a variety of machine learning algorithms. Of these, the GBTClassifier algorithm emerged as the most promising, and we proceeded to refine the model until we achieved an accuracy and F1 score of\u00a00.69.</p>\n<h4>Impact</h4>\n<p>Sparkify can utilize the obtained insights to pinpoint customers who have a high likelihood of churning and offer them appealing incentives to remain with the service. This can lead to a reduction in revenue loss for Sparkify while simultaneously providing a favorable deal to customers. Given that newer customers are more prone to churning, offering them a complimentary trial of the premium service without advertisements could be an effective strategy. Ultimately, predicting and addressing potential churn can result in significant cost savings for both the business and customers alike. It could also make a crucial difference in preventing the loss of the business to the ever-competitive market.</p>\n<h3>Future direction</h3>\n<p>Improvements:</p>\n<ul>\n<li>Feature engineering can be more in\u00a0depth</li>\n<li>The full dataset containing 12G of customer info can be considered using AWS cloud\u00a0system</li>\n<li>Instead of predicting which cstomer will churn and which not, we can employ a real-time prediction model in which by each interaction the model output a prediction probability in near future (i.e., next 2\u00a0days).</li>\n<li>Include more variables in the\u00a0analysis</li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5e61230c338d\" width=\"1\" height=\"1\" alt=\"\">\n",
      "content": "\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/841/1*jfqNg7QD_g7GVAnL-bv_LQ.jpeg\"></figure><h3>Project motivation</h3>\n<p>One issue that frequently arises in service-based businesses, such as Spotify, is the need to anticipate churn. Churn refers to the departure of customers, and it\u2019s a crucial metric to keep track of. By anticipating churn, these companies can take preventative measures to avoid customer loss, which can ultimately lead to an increase in customer base and\u00a0revenue.</p>\n<p>Udacity has created Sparkify, a simulated music streaming platform that generates data similar to real-world companies like Spotify and Pandora. Each day, millions of users listen to their favorite songs via music streaming services, either on a free tier plan with ads or on a premium subscription model without ads and with additional features. Users can modify their subscription plan at any time or cancel it completely, making it vital to ensure customer satisfaction. Whenever a user interacts with a music streaming app, data is generated, including playing songs, adding them to playlists, rating them, adding friends, logging in/out, and adjusting settings. The user activity logs provide valuable insights into whether users are happy with the\u00a0service.</p>\n<h3>Introduction to\u00a0data</h3>\n<p>As a part of this task, we will be utilizing event data that has been provided to us. There are two datasets at our disposal for analysis, one of considerable size and another that is comparatively smaller. The full dataset contains approximately 12GB of data and could be used by those who wish to deploy their model on AWS or any other cloud service. For the purposes of this task, I will be working with the moderately-sized dataset, which contains 123MB of data. For a more detailed analysis and information on the prerequisites, please refer to <a href=\"https://github.com/Mehranmzn/DataScience-Nanodegree/tree/master/notebooks/Project%204\">my Github</a>\u00a0repo.</p>\n<h3>Exploratory Data\u00a0Analysis</h3>\n<p>To start with, our initial task was to come up with a clear definition of what \u201cchurn\u201d meant for our analysis. This involved creating a new column, called Churn, which served as a label for our model. We determined whether a customer had churned or not based on whether they had accessed the Cancellation Confirmation page, which was included as a feature in our dataset. Specifically, we labeled customers as \u201c1\u201d if they had churned and \u201c0\u201d if they had remained with Sparkify. With this label in place, we could then dive deeper into our data and identify key features that would be most valuable in predicting churn.</p>\n<h4>Frequency of\u00a0Churn</h4>\n<p>The table presented below illustrates that there were a total of 173 Sparkify customers who chose to stay, while 52 customers decided to churn. Therefore, we can deduce that 23% of the subset of Sparkify\u2019s customers had churned. It is worth mentioning that this dataset is imbalanced, which is an important factor to consider as we continue with our analysis.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/166/1*-0nZD-J_lm9t91SGNfNDEw.png\"></figure><h3>Load data and\u00a0Cleaning</h3>\n<p>The dataset consists of 286500 rows and 18 columns, with each user having multiple rows that serve as log entries for each session. While most of the columns are self-explanatory, it\u2019s worth highlighting some crucial features that will be used for our analysis.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/846/1*TnadJ1PN_pOTS3TQMpxZ0w.png\"><figcaption>Columns in the\u00a0data</figcaption></figure><p>Where:</p>\n<p>Artist: The name of the artist whose song was played during the event. This field is null if the event is not related to song listening.</p>\n<p>Authentication (Auth): The status of user authentication, such as login, logout, cancel, or\u00a0guest.</p>\n<p>First Name: The first name of the user. This field is null if the user has not logged\u00a0in.</p>\n<p>Gender: The gender of the user, either male or female. This field is null if the user has not logged\u00a0in.</p>\n<p>Item in Session: The count of events that occurred during a particular session. The count starts from\u00a00.</p>\n<p>Last Name: The last name of the user. This field is null if the user has not logged\u00a0in.</p>\n<p>Length: The duration in seconds of the song being played. This field is null if the event is not related to song listening.</p>\n<p>Level: The subscription level of the user, whether paid or\u00a0free.</p>\n<p>Location: The location of the user in the format of city and state. All users are from the\u00a0USA.</p>\n<p>Page: The page where the event occurred, which can be Cancel, Submit Downgrade, Thumbs Down, Home, Downgrade, Roll Advert, Logout, Save Settings, Cancellation Confirmation, About, Submit Registration, Settings, Login, Register, Add to Playlist, Add Friend, NextSong, Thumbs Up, Help, Upgrade, Error and Submit\u00a0Upgrade.</p>\n<p>Registration: The registration number associated with each\u00a0user.</p>\n<p>Session Id: The number of the session initiated.</p>\n<p>Song: The name of the song being\u00a0played.</p>\n<p>Status: The HTTP status code (200, 307,\u00a0404).</p>\n<p>Timestamp (TS): The timestamp in milliseconds.</p>\n<p>User Agent: Information about the device and browser accessing the\u00a0data.</p>\n<p>User Id: The ID associated with each\u00a0user.</p>\n<p>Upon examining the records, it is observed that a user typically spends a considerable amount of time using the system as a paid user before ultimately ending their subscription. In certain cases, they continue to use the service as a free user. Our objective is to forecast when this event of canceling or ending the subscription will\u00a0occur.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/490/1*Xfl5GhFDXU5H6ajbTQeMbw.png\"><figcaption>Number of artists in the\u00a0dataset</figcaption></figure><p>Churn defined when a user cancel the memebeship from paid to either non-paid or leave the platform.</p>\n<p>To gain a deeper comprehension of churn behavior, we will examine additional columns to identify any significant information within or between them. As a result, I have created a distribution plot of song\u00a0lengths.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/681/1*XSsgQYmS09aoUdckQ1hSJw.png\"><figcaption>Distribution of songs\u00a0length</figcaption></figure><p>Churn rate based on gender, level of membership, and page interaction, given in the below\u00a0figure:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*BAXYaFJhVjIwcCmvZgoPIw.png\"><figcaption>Red bars representing the churn users vs the green bars are non-churned users</figcaption></figure><p>Considering these features, there isn\u2019t a great deal of information to report. Therefore, it would be worthwhile to examine the frequency of page visits for both Churn and non-Churn users.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*zWrRykB5tVbsWn0ePrPSDA.png\"></figure><p>Once again, there doesn\u2019t appear to be a distinct differentiation between Churn and non-Churn users. Next, let\u2019s examine the following figure which illustrates the amount of time spent listening to songs by these two\u00a0groups.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*xxJgLXKyrrnwr1UOc7zd4w.png\"></figure><p>In this case, there is a noticeable distinction between Churn and non-Churn users. It is apparent that users who exhibit Churn behavior listen to a lower quantity of songs compared to the other group. Let\u2019s now shift our attention to the web browsers\u00a0used.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*QMuN6ZioFOKrZVdjBS5bUw.png\"></figure><p>There seems to be more drop out in Chrome than other web browers which could be due to the fact that most of the customer used\u00a0Chrome.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*MCwRPWp4IYESa_0oSMkSBQ.png\"></figure><p>It appears that the average number of days since registration is relatively similar for both groups, but the variance among users in the non-Churn group is\u00a0higher.</p>\n<h3>Feature Engineering</h3>\n<p>I have taken several steps to transform the original dataset to a format that is understandable for modeling part. These steps\u00a0are:</p>\n<ol>\n<li>Gender will be transformed from Male/Female to\u00a0binary</li>\n<li>Level of memebership: e.g., paid will be 1 otherwise 0</li>\n<li>Artists columns: Number of artisits listen to for each unique\u00a0user</li>\n<li>Average number of song a user listen\u00a0to</li>\n<li>Number of times that a user click on the rolling\u00a0ad</li>\n<li>Number of Thums up and Thums down that each user\u00a0did</li>\n<li>Number of times a user add a\u00a0friend</li>\n<li>Number of times a user log out from the\u00a0platform</li>\n<li>Churn will be binary when a Cancellation is confirmed in\u00a0page.</li>\n</ol>\n<p>Once these datasets merged, we have the final, ready to train\u00a0dataset:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*FpXOUux0RDzxSc3_SlTfwQ.png\"></figure><h3>Data preprocessing</h3>\n<p>We need all the columns to be float. So First all the columns type changed to be\u00a0float.</p>\n<h4>Vectorizing</h4>\n<p>Afterward, we employed the VectorAssembler module of Spark to condense the feature space into a vector. Ensuring that features with high values do not overpower the results is crucial for our machine learning\u00a0model.</p>\n<pre>assembler = VectorAssembler(inputCols=['gender', 'level', 'number_artist_listen', 'p_list',<br>                                      'thumbsup', 'thumsdown', 'dgrade', 'nsong', 'ad'],<br>                            outputCol='features_vec')</pre>\n<h4>Standard Scaler</h4>\n<p>The values should be standarized to be sure that they have a similar\u00a0scale.</p>\n<pre># Fit StandardScaler model on assembled data<br>scaler = StandardScaler(inputCol='features_vec', outputCol='scaled_features')<br>scaler_model = scaler.fit(df_assembled)<br><br># Apply scaler model on assembled data<br>df_scaled = scaler_model.transform(df_assembled)</pre>\n<h4>Train-test split</h4>\n<p>Following this, we performed a train-test split of 80/20, which resulted in a training set of 173 and a testing set of\u00a052.</p>\n<h3>Modelling</h3>\n<p>After constructing the features dataFrame with exclusively numeric variables, we proceeded to divide the entire dataset into three subsets: train, test, and validation. We subsequently experimented with various machine learning classification algorithms, including:</p>\n<ul>\n<li>LogisticRegression</li>\n<li>RandomForestClassifier</li>\n<li>GBTClassifier</li>\n<li>NaiveBayes</li>\n</ul>\n<p>In all the methods the target variable (Churn) is a binary variabel so the task is bainary classification.</p>\n<h4>Evaluation Metrics</h4>\n<p>We will assess the performance of the models using two evaluation metrics: accuracy and F1score. F1score is being utilized due to the unbalanced nature of the dataset, with only 52 customers having churned. Our first model to be tested was the tried-and-true logistic regression.</p>\n<h4>LogisticRegression</h4>\n<p>The binary classification problem at hand can benefit from the application of logistic regression, a dependable machine learning algorithm that offers a transparent model. Logistic regression is straightforward to put into practice and interpret, and it boasts a high level of efficiency in training. Additionally, it has a lower tendency to overfit, making it a favorable option.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/940/1*kTvHJHE0fovhfD4-ILFvhw.png\"></figure><h4>RandomForestClassifier</h4>\n<p>The ensemble technique known as Random Forest (RF) builds numerous decision trees to generate predictions, and subsequently aggregates the decisions through a majority vote. By doing so, RF can mitigate the risk of overfitting. Furthermore, RF is a robust algorithm that excels in handling imbalanced datasets, which is pertinent to the current scenario. The performance of RF on such datasets is noteworthy.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1002/1*n2H5c3Allg3ZWaPij_oOPw.png\"></figure><h4>GBTClassifier</h4>\n<p>The Gradient Boosting Trees (GBT) approach constructs decision trees sequentially, with each subsequent tree rectifying the errors of its predecessor. In contrast, Random Forest (RF) creates trees independently. While GBT has the potential to overfit, this must be taken into account. On the positive side, GBT exhibits strong performance in dealing with unbalanced data, which is relevant to the present circumstance.We used several models, including\u00a0, and evaluated them based on accuracy and\u00a0F1score.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/830/1*NouvaVj3sPQnC4QNn3jBsw.png\"></figure><h4>NaiveBayes</h4>\n<p>A fast algorithm for our classfying tasks.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1002/1*DayxofGhxUCUJPdCavBgJQ.png\"></figure><p>The result showed that best performing model was GBTClassifier with accuracy of 0.69 and F1 score of\u00a00.73.</p>\n<h3>Conclusion</h3>\n<p>Our project began with a relatively small dataset of only 128 megabytes (The original dataset of this captone is 12G and becuase of limitation of resources, we worked on a small subset of the data) and 225 distinct customers. Upon importing and refining the data, we conducted a thorough analysis of the dataset to identify pertinent predictors of churn, ultimately isolating the most effective features. These selected features were subjected to preprocessing and subsequently employed as input variables in a variety of machine learning algorithms. Of these, the GBTClassifier algorithm emerged as the most promising, and we proceeded to refine the model until we achieved an accuracy and F1 score of\u00a00.69.</p>\n<h4>Impact</h4>\n<p>Sparkify can utilize the obtained insights to pinpoint customers who have a high likelihood of churning and offer them appealing incentives to remain with the service. This can lead to a reduction in revenue loss for Sparkify while simultaneously providing a favorable deal to customers. Given that newer customers are more prone to churning, offering them a complimentary trial of the premium service without advertisements could be an effective strategy. Ultimately, predicting and addressing potential churn can result in significant cost savings for both the business and customers alike. It could also make a crucial difference in preventing the loss of the business to the ever-competitive market.</p>\n<h3>Future direction</h3>\n<p>Improvements:</p>\n<ul>\n<li>Feature engineering can be more in\u00a0depth</li>\n<li>The full dataset containing 12G of customer info can be considered using AWS cloud\u00a0system</li>\n<li>Instead of predicting which cstomer will churn and which not, we can employ a real-time prediction model in which by each interaction the model output a prediction probability in near future (i.e., next 2\u00a0days).</li>\n<li>Include more variables in the\u00a0analysis</li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5e61230c338d\" width=\"1\" height=\"1\" alt=\"\">\n",
      "enclosure": {},
      "categories": [
        "udacity-nanodegree",
        "python",
        "sparkify",
        "churn-prediction",
        "data-science"
      ]
    },
    {
      "title": "Top/poor hosts in Seatle Airbnb dataset. What factors are important and why?",
      "pubDate": "2023-02-22 21:31:08",
      "link": "https://medium.com/@mehran1414/top-poor-hosts-in-seatle-airbnb-dataset-what-factors-are-important-and-why-fd5535f1d96d?source=rss-3855f4d29cc4------2",
      "guid": "https://medium.com/p/fd5535f1d96d",
      "author": "Mehran Moazeni",
      "thumbnail": "",
      "description": "\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*cXtncAyQa3kcRsIlzGmZOg.png\"><figcaption>DALLE generated image for Seatle Airbnb\u00a0dataset</figcaption></figure><p>Have you ever wondered what makes a successful Airbnb host in\u00a0Seattle?</p>\n<p>Or what factors influence the pricing of Airbnb listings in the\u00a0city?</p>\n<p>As an avid traveler and data enthusiast, I decided to explore these questions using the Seattle Airbnb dataset available on Kaggle. In this blog post, I\u2019ll share my findings on how top and low performers differ when it comes to listing their homes, as well as some insights on the pricing strategies of successful hosts. So whether you\u2019re a Seattle resident looking to become an Airbnb host or a curious traveler interested in the local market, read on to learn more about the fascinating world of Airbnb in the Emerald\u00a0City.</p>\n<p>In summary, I will\u00a0cover:</p>\n<ul>\n<li>Characteristics of a top-performing hosts apart from their less successful counterparts</li>\n<li>Differences in pricing between successful and unsuccessful hosts</li>\n</ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*AI7DYS35Yc6hkc9HNCYODw.png\"><figcaption>DALLE generated image</figcaption></figure><p>First, I used the listing.csv file from the Kaggle repository to make a distinction between succesful (Good) performer and unsuccesful (Bad) performer in Airbnb. For this purpose, we have on column with reviews_per_monthand another column for scores_rating, which shoul dbe combined to make a new score. I liked the strategy that YOGI CAHYONO on his notebook on Kaggle followed:</p>\n<p>Performance_score = reviews_per_month * scores_rating/10</p>\n<p>With this function, the new score ranges between 0 to 115.425. Now to seperate A Gperformer form a Bperformer, I will set a boundry on the new score. This threshold is based on 90% quantile of Gperformer and 20% of quantile for Bperformer. Then:</p>\n<pre>Threshold for best performer listings: 41.27720000000001<br>Threshold for bad performer listings: 1.6764000000000006</pre>\n<p>Although this number is important to us, we should also examine the distribution of the new score to investigate how it is distributed and what proportion of performers are classified as G or B. To accomplish this, I have plotted a bar chart containing the ranges of the new score with three colors representing G and B performers as well as other categories (based on YOGI CAHYONO\u2019s notebook).</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/620/1*i0rnxab4MDTkFWoL2S-U1A.png\"></figure><p>The plot represents that category of Bperformer has the most listings, and Gperformers category starts from 42 score with lower number of listings.</p>\n<p>Next, I was interested in examining the relationship between other factors that may differ between G and B performers. Ultimately, we want to identify which factors lead to success or failure as a performer. To accomplish this, I have selected the following features and will compare them based on their correlation values.</p>\n<pre>selected_feaures = [<br>'review_scores_rating', <br>'review_scores_accuracy', <br>'review_scores_cleanliness',<br> 'review_scores_checkin',<br> 'review_scores_communication',<br> 'review_scores_location',<br> 'review_scores_value'<br>]<br></pre>\n<p>Applying the cluster correlation results in\u00a0:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/708/1*HbRofA-cDqNS7NfGRfxTpw.png\"></figure><p>So the correlation plot suggests us that review_scores_rating is highly correlated with review_scores_value (which could be obvious!). Also, it is intersting that review_scores_checkin are highly correlated with review_scores_communication where we can assume that better communication means better checkin experiences.</p>\n<p>There are also other features that may impact the charactersitics of G&amp;B performer.</p>\n<pre>selected_features_2=[<br>'Acceptance Rate',<br> 'N. of Identified Listings',<br> 'Superhost Listings',<br> 'Instant Bookable Feature', <br>'How Long Hosts Will Respond?',<br> 'Responses Probability'<br>]</pre>\n<p>And with the help of bar plots we can see the differences between these two\u00a0groups:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1021/1*rciZbK-nJihTfrOaoYeCWQ.png\"></figure><p>It\u2019s interesting to note that timely responses from hosts are appreciated by guests (G performers usually respond within an hour). Additionally, G performers tend to have more N.listings` listings than B performers. However, B performers outperform G performers in terms of quantity when it comes to other features.</p>\n<p>Reagrding the propoerty type for each of G&amp;B performers:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/893/1*FY5moR8BDzXGgOVZpO6eVg.png\"><figcaption>Green: Gperformer, Blue: Bperformer</figcaption></figure><p>We could conclude that Gperformers listings are mostly Condominium and Chalet, while Bperformers commodity types scattered on different range of\u00a0types.</p>\n<p>Now lets discusse the second question, How price differe in each of these\u00a0groups?</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/729/1*5aanmtt1xuZa6Cl_EyZfDA.png\"></figure><p>Wow! it seems that Bperfomers are offering more price on their propoerty. This is even the case in Condominium and Chalet cases where Gperformers have more properties.</p>\n<p><strong>Conclusion and wrap\u00a0up</strong></p>\n<p>I found that in terms of pricing, successful Airbnb hosts offer lower rates on their properties than unsuccessful ones. A successful host is defined as someone who has received a score of 42 or higher based on their reviews and ratings per month. It\u2019s also worth mentioning that a high-performing (G) host responds to requests within an hour compared to a low-performing (B)\u00a0host.</p>\n<p>This blog and the <a href=\"https://github.com/Mehranmzn/DataScience-Nanodegree/blob/master/notebooks/Project%201/Untitled.ipynb\">GitHub</a> repository serve as an introduction to analyzing this dataset, and there are certainly areas for improvement.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=fd5535f1d96d\" width=\"1\" height=\"1\" alt=\"\">\n",
      "content": "\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*cXtncAyQa3kcRsIlzGmZOg.png\"><figcaption>DALLE generated image for Seatle Airbnb\u00a0dataset</figcaption></figure><p>Have you ever wondered what makes a successful Airbnb host in\u00a0Seattle?</p>\n<p>Or what factors influence the pricing of Airbnb listings in the\u00a0city?</p>\n<p>As an avid traveler and data enthusiast, I decided to explore these questions using the Seattle Airbnb dataset available on Kaggle. In this blog post, I\u2019ll share my findings on how top and low performers differ when it comes to listing their homes, as well as some insights on the pricing strategies of successful hosts. So whether you\u2019re a Seattle resident looking to become an Airbnb host or a curious traveler interested in the local market, read on to learn more about the fascinating world of Airbnb in the Emerald\u00a0City.</p>\n<p>In summary, I will\u00a0cover:</p>\n<ul>\n<li>Characteristics of a top-performing hosts apart from their less successful counterparts</li>\n<li>Differences in pricing between successful and unsuccessful hosts</li>\n</ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*AI7DYS35Yc6hkc9HNCYODw.png\"><figcaption>DALLE generated image</figcaption></figure><p>First, I used the listing.csv file from the Kaggle repository to make a distinction between succesful (Good) performer and unsuccesful (Bad) performer in Airbnb. For this purpose, we have on column with reviews_per_monthand another column for scores_rating, which shoul dbe combined to make a new score. I liked the strategy that YOGI CAHYONO on his notebook on Kaggle followed:</p>\n<p>Performance_score = reviews_per_month * scores_rating/10</p>\n<p>With this function, the new score ranges between 0 to 115.425. Now to seperate A Gperformer form a Bperformer, I will set a boundry on the new score. This threshold is based on 90% quantile of Gperformer and 20% of quantile for Bperformer. Then:</p>\n<pre>Threshold for best performer listings: 41.27720000000001<br>Threshold for bad performer listings: 1.6764000000000006</pre>\n<p>Although this number is important to us, we should also examine the distribution of the new score to investigate how it is distributed and what proportion of performers are classified as G or B. To accomplish this, I have plotted a bar chart containing the ranges of the new score with three colors representing G and B performers as well as other categories (based on YOGI CAHYONO\u2019s notebook).</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/620/1*i0rnxab4MDTkFWoL2S-U1A.png\"></figure><p>The plot represents that category of Bperformer has the most listings, and Gperformers category starts from 42 score with lower number of listings.</p>\n<p>Next, I was interested in examining the relationship between other factors that may differ between G and B performers. Ultimately, we want to identify which factors lead to success or failure as a performer. To accomplish this, I have selected the following features and will compare them based on their correlation values.</p>\n<pre>selected_feaures = [<br>'review_scores_rating', <br>'review_scores_accuracy', <br>'review_scores_cleanliness',<br> 'review_scores_checkin',<br> 'review_scores_communication',<br> 'review_scores_location',<br> 'review_scores_value'<br>]<br></pre>\n<p>Applying the cluster correlation results in\u00a0:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/708/1*HbRofA-cDqNS7NfGRfxTpw.png\"></figure><p>So the correlation plot suggests us that review_scores_rating is highly correlated with review_scores_value (which could be obvious!). Also, it is intersting that review_scores_checkin are highly correlated with review_scores_communication where we can assume that better communication means better checkin experiences.</p>\n<p>There are also other features that may impact the charactersitics of G&amp;B performer.</p>\n<pre>selected_features_2=[<br>'Acceptance Rate',<br> 'N. of Identified Listings',<br> 'Superhost Listings',<br> 'Instant Bookable Feature', <br>'How Long Hosts Will Respond?',<br> 'Responses Probability'<br>]</pre>\n<p>And with the help of bar plots we can see the differences between these two\u00a0groups:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1021/1*rciZbK-nJihTfrOaoYeCWQ.png\"></figure><p>It\u2019s interesting to note that timely responses from hosts are appreciated by guests (G performers usually respond within an hour). Additionally, G performers tend to have more N.listings` listings than B performers. However, B performers outperform G performers in terms of quantity when it comes to other features.</p>\n<p>Reagrding the propoerty type for each of G&amp;B performers:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/893/1*FY5moR8BDzXGgOVZpO6eVg.png\"><figcaption>Green: Gperformer, Blue: Bperformer</figcaption></figure><p>We could conclude that Gperformers listings are mostly Condominium and Chalet, while Bperformers commodity types scattered on different range of\u00a0types.</p>\n<p>Now lets discusse the second question, How price differe in each of these\u00a0groups?</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/729/1*5aanmtt1xuZa6Cl_EyZfDA.png\"></figure><p>Wow! it seems that Bperfomers are offering more price on their propoerty. This is even the case in Condominium and Chalet cases where Gperformers have more properties.</p>\n<p><strong>Conclusion and wrap\u00a0up</strong></p>\n<p>I found that in terms of pricing, successful Airbnb hosts offer lower rates on their properties than unsuccessful ones. A successful host is defined as someone who has received a score of 42 or higher based on their reviews and ratings per month. It\u2019s also worth mentioning that a high-performing (G) host responds to requests within an hour compared to a low-performing (B)\u00a0host.</p>\n<p>This blog and the <a href=\"https://github.com/Mehranmzn/DataScience-Nanodegree/blob/master/notebooks/Project%201/Untitled.ipynb\">GitHub</a> repository serve as an introduction to analyzing this dataset, and there are certainly areas for improvement.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=fd5535f1d96d\" width=\"1\" height=\"1\" alt=\"\">\n",
      "enclosure": {},
      "categories": ["airbnb", "data-analysis", "analytics", "data-science"]
    }
  ]
}
